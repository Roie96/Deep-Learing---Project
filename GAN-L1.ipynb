{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-16T18:41:45.699051Z","iopub.status.busy":"2024-04-16T18:41:45.698159Z","iopub.status.idle":"2024-04-16T18:41:53.192633Z","shell.execute_reply":"2024-04-16T18:41:53.191572Z","shell.execute_reply.started":"2024-04-16T18:41:45.699017Z"},"id":"rmyCr3OgROUT","trusted":true},"outputs":[],"source":["from torchvision.datasets import Flowers102\n","import multiprocessing\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import os\n","import pandas as pd\n","from google.colab import drive\n","\n","\n","num_workers = multiprocessing.cpu_count()\n","print(\"Number of CPU cores available:\", num_workers)\n","batch_size = 4\n","\n","class DataProvider(Dataset):\n","\n","    def __init__(self, sample_dataset, label_dataset):\n","        self.sample_dataset = sample_dataset\n","        self.label_dataset = label_dataset\n","\n","    def __len__(self):\n","        return len(self.sample_dataset)\n","\n","    def __getitem__(self, idx):\n","        sample = self.sample_dataset[idx][0]\n","        label = self.label_dataset[idx][0]\n","        return (sample, label)\n","\n","\n","transform_Grayscale = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.Grayscale(),\n","    transforms.ToTensor()\n","])\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor()\n","])\n","\n","# # Load sample datasets rgb image\n","train_set_sample = Flowers102(root='.', split='train', download=True, transform=transform)\n","test_set_sample = Flowers102(root='.', split='test', download=True, transform=transform)\n","validation_set_sample = Flowers102(root='.', split='val', download=True, transform=transform)\n","\n","# # Load label datasets - grayscale images as labels)\n","train_set_label = Flowers102(root='.', split='train', download=True, transform=transform_Grayscale)\n","test_set_label = Flowers102(root='.', split='test', download=True, transform=transform_Grayscale)\n","validation_set_label = Flowers102(root='.', split='val', download=True, transform=transform_Grayscale)\n","\n","train_set = DataProvider(train_set_sample, train_set_label)\n","test_set = DataProvider(test_set_sample, test_set_label)\n","validation_set = DataProvider(validation_set_sample, validation_set_label)\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Device:', device)"]},{"cell_type":"markdown","metadata":{"id":"RriO1C6qROUa"},"source":["# Generator - UNET"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:42:13.614686Z","iopub.status.busy":"2024-04-16T18:42:13.614263Z","iopub.status.idle":"2024-04-16T18:42:13.636464Z","shell.execute_reply":"2024-04-16T18:42:13.635380Z","shell.execute_reply.started":"2024-04-16T18:42:13.614649Z"},"id":"Z9eFuvj_ROUd","trusted":true},"outputs":[],"source":["class VGGBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n","        super(VGGBlock, self).__init__()\n","        layers = [\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        ]\n","       \n","        if dropout_rate > 0.0:\n","            layers.append(nn.Dropout(dropout_rate))\n","        \n","        self.conv = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNetGenerator(nn.Module):\n","    def __init__(self):\n","        super(UNetGenerator, self).__init__()\n","        \n","        # Pooling and upsampling\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        \n","        # Encoder: Downsampling path\n","        self.encoder_block1 = VGGBlock(1, 64)\n","        self.encoder_block2 = VGGBlock(64, 128)\n","        self.encoder_block3 = VGGBlock(128, 256)\n","        self.encoder_block4 = VGGBlock(256, 512)\n","        self.encoder_block5 = VGGBlock(512, 512)\n","        self.encoder_block6 = VGGBlock(512, 1024)\n","        \n","        # Decoder: Upsampling path\n","        self.decoder_block5 = VGGBlock(512 + 1024, 512)\n","        self.decoder_block4 = VGGBlock(512 + 512, 256)\n","        self.decoder_block3 = VGGBlock(256 + 256, 256)\n","        self.decoder_block2 = VGGBlock(128 + 256, 128)\n","        self.decoder_block1 = VGGBlock(128 + 64, 64)\n","        \n","        # Final convolution\n","        self.conv_last = nn.Conv2d(64, 3, kernel_size=1)\n","        \n","    def forward(self, x):\n","        # Downsample\n","        conv1 = self.encoder_block1(x)\n","        conv2 = self.encoder_block2(self.maxpool(conv1))\n","        conv3 = self.encoder_block3(self.maxpool(conv2))\n","        conv4 = self.encoder_block4(self.maxpool(conv3))\n","        conv5 = self.encoder_block5(self.maxpool(conv4))\n","        x = self.encoder_block6(self.maxpool(conv5))\n","\n","        # Upsample and concatenate\n","        x = torch.cat([self.upsample(x), conv5], dim=1)\n","        x = torch.cat([self.upsample(self.decoder_block5(x)), conv4], dim=1)       \n","        x = torch.cat([self.upsample(self.decoder_block4(x)), conv3], dim=1)     \n","        x = torch.cat([self.upsample(self.decoder_block3(x)), conv2], dim=1)            \n","        x = torch.cat([self.upsample(self.decoder_block2(x)), conv1], dim=1)   \n","        \n","        # Final convolution\n","        out = self.conv_last(self.decoder_block1(x))\n","        \n","        return out"]},{"cell_type":"markdown","metadata":{"id":"wibohoHIROUe"},"source":["# Discriminator - CNN"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:42:13.637937Z","iopub.status.busy":"2024-04-16T18:42:13.637582Z","iopub.status.idle":"2024-04-16T18:42:13.656340Z","shell.execute_reply":"2024-04-16T18:42:13.655349Z","shell.execute_reply.started":"2024-04-16T18:42:13.637909Z"},"id":"q9Zx8-q7ROUf","trusted":true},"outputs":[],"source":["class VGG_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n","        super(VGG_block, self).__init__()\n","        layers = [\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        ]\n","        \n","        if dropout_rate > 0.0:\n","            layers.append(nn.Dropout(dropout_rate))\n","        \n","        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n","        \n","        self.block = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.block(x)\n","    \n","    \n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","                \n","        self.layer1 = VGG_block(3, 64)  \n","        self.layer2 = VGG_block(64, 128)\n","        self.layer3 = VGG_block(128, 256)\n","        self.layer4 = VGG_block(256, 512)\n","        self.layer5 = VGG_block(512, 1024)\n","        self.layer6 = VGG_block(1024, 1024)\n","        \n","        # Final convolutional layer\n","        self.final_conv = nn.Sequential(\n","            nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=1, bias=False)\n","        )\n","\n","    def forward(self, img):\n","        x1 = self.layer1(img)\n","        x2 = self.layer2(x1)\n","        x3 = self.layer3(x2)\n","        x4 = self.layer4(x3)\n","        x5 = self.layer5(x4)\n","        x6 = self.layer6(x5)\n","        output = self.final_conv(x6)\n","        return torch.sigmoid(output)  "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:42:13.657932Z","iopub.status.busy":"2024-04-16T18:42:13.657610Z","iopub.status.idle":"2024-04-16T18:42:14.843497Z","shell.execute_reply":"2024-04-16T18:42:14.842528Z","shell.execute_reply.started":"2024-04-16T18:42:13.657906Z"},"id":"pvdMK8GMROUh","trusted":true},"outputs":[],"source":["# Initialize the generator and discriminator\n","generator = UNetGenerator()  \n","discriminator = Discriminator()\n","\n","optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","adversarial_loss = nn.BCEWithLogitsLoss()\n","mse_loss = nn.MSELoss()\n","l1_loss = nn.L1Loss()\n","\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:42:14.844956Z","iopub.status.busy":"2024-04-16T18:42:14.844698Z","iopub.status.idle":"2024-04-16T18:42:14.858629Z","shell.execute_reply":"2024-04-16T18:42:14.857443Z","shell.execute_reply.started":"2024-04-16T18:42:14.844934Z"},"id":"KfJiv7yhROUj","trusted":true},"outputs":[],"source":["def psnr(image_true, image_pred):\n","    mse = F.mse_loss(image_pred, image_true)\n","    max_pixel = 1.0\n","    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n","\n","def ssim(img1, img2, C1=0.01**2, C2=0.03**2):\n","    mean1 = img1.mean([2, 3])\n","    mean2 = img2.mean([2, 3])\n","    std1 = img1.std([2, 3])\n","    std2 = img2.std([2, 3])\n","    std12 = (img1 * img2).mean([2, 3]) - mean1 * mean2\n","    \n","    ssim_n = (2 * mean1 * mean2 + C1) * (2 * std12 + C2)\n","    ssim_d = (mean1**2 + mean2**2 + C1) * (std1**2 + std2**2 + C2)\n","    \n","    return ssim_n / ssim_d\n","\n","def validate(generator, criterion, validation_loader):\n","    generator.eval()  \n","    total_loss = 0\n","    total_psnr = 0\n","    total_mae = 0\n","    total_ssim = 0\n","\n","    with torch.no_grad():\n","        for real_image, grey_image in validation_loader:\n","            grey_image = grey_image.to(device)\n","            real_image = real_image.to(device)\n","            # Generate fake images\n","            fake_image = generator(grey_image)\n","\n","            # Compute MSE loss\n","            loss = criterion(fake_image, real_image)\n","            # Accumulate total loss\n","            total_loss += loss.item()\n","\n","            # Compute PSNR\n","            psnr_value = psnr(real_image, fake_image)\n","            total_psnr += psnr_value.item()\n","\n","            #\n","            ssim_value = ssim(real_image, fake_image)\n","            total_ssim += ssim_value.mean().item()\n","            \n","            # Compute MAE\n","            mae_value = F.l1_loss(fake_image, real_image)\n","            total_mae += mae_value.item()\n","\n","    # Calculate average validation loss and other metrics\n","    avg_loss = total_loss / len(validation_loader)\n","    avg_psnr = total_psnr / len(validation_loader)\n","    avg_ssim = total_ssim / len(validation_loader)\n","    avg_mae = total_mae / len(validation_loader)\n","\n","    return avg_loss, avg_psnr, avg_ssim, avg_mae"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T18:42:14.861490Z","iopub.status.busy":"2024-04-16T18:42:14.861117Z","iopub.status.idle":"2024-04-16T18:42:14.878879Z","shell.execute_reply":"2024-04-16T18:42:14.877841Z","shell.execute_reply.started":"2024-04-16T18:42:14.861456Z"},"id":"TBPSXPPbROUm","trusted":true},"outputs":[],"source":["# Hyperparameters\n","batch_epoch = 15\n","lambda_adv = 0.01\n","lambda_mse = 0.99\n","\n","# Training loop\n","num_epochs = 350\n","save_every_epoch = 35\n","\n","# Lists to store losses\n","train_losses_discriminator = []\n","train_losses_generator = []\n","mse_losses = []\n","validation_losses = []\n","psnr_values = []\n","mae_values = []\n","ssim_values = []"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-16T18:42:14.882062Z","iopub.status.busy":"2024-04-16T18:42:14.881755Z","iopub.status.idle":"2024-04-17T03:56:02.424005Z","shell.execute_reply":"2024-04-17T03:56:02.422703Z","shell.execute_reply.started":"2024-04-16T18:42:14.882037Z"},"id":"XX7atmu8ROUm","outputId":"4959e699-f45b-4db8-8830-88ad944db1e1","trusted":true},"outputs":[],"source":["total_discriminator_loss = 0.0\n","total_generator_loss = 0.0\n","total_mse_loss = 0.0\n","\n","for epoch in range(num_epochs):\n","    generator.train()\n","    discriminator.train()\n","\n","    for idx, (real_image, gray_image) in enumerate(train_loader):\n","        batch_size = gray_image.size(0)\n","        \n","        gray_image = gray_image.to(device)\n","        real_image = real_image.to(device)\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","        optimizer_D.zero_grad()\n","        \n","        # Train discriminator with real images\n","        real_predictions = discriminator(real_image)\n","        real_labels = torch.ones_like(real_predictions).to(device) # Real labels\n","        real_loss = adversarial_loss(real_predictions, real_labels)\n","\n","        # Train discriminator with fake images\n","        fake_color_image = generator(gray_image).detach()\n","        fake_predictions = discriminator(fake_color_image) \n","        fake_labels = torch.zeros_like(fake_predictions).to(device)  # Label for fake images\n","        fake_loss = adversarial_loss(fake_predictions, fake_labels)\n","\n","        # Total discriminator loss\n","        discriminator_loss = real_loss + fake_loss\n","\n","        # Update discriminator weights\n","        discriminator_loss.backward()\n","        optimizer_D.step()\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","    \n","        optimizer_G.zero_grad()\n","        fake_image = generator(gray_image)\n","        fake_predictions = discriminator(fake_image)\n","        \n","        \n","        real_labels = torch.ones_like(fake_predictions).to(device) # Label for real images\n","        generator_loss_adv = adversarial_loss(fake_predictions, real_labels)\n","        \n","        # Compute L1 loss between generated color images and ground truth\n","        generator_loss_l1 = l1_loss(fake_image, real_image)\n","        \n","         # Combine adversarial loss and MSE loss\n","        generator_loss =  lambda_adv * generator_loss_adv + lambda_mse * generator_loss_l1\n","\n","\n","        # Update generator weights\n","        generator_loss.backward()\n","        optimizer_G.step()\n","\n","        train_losses_discriminator.append(discriminator_loss.item())\n","        train_losses_generator.append(generator_loss.item())\n","        mse_losses.append(generator_loss_l1.item())\n","\n","\n","        if idx % batch_epoch == 0:\n","            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{idx}/{len(train_loader)}], Generator GAN Loss: {generator_loss.item():.4f}, Generator L1 Loss: {generator_loss_l1.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}\")\n","\n","    # Calculate validation loss\n","    validation_loss, validation_psnr, validation_ssim, validation_mae = validate(generator, mse_loss, validation_loader)\n","\n","    validation_losses.append(validation_loss)\n","    psnr_values.append(validation_psnr)\n","    mae_values.append(validation_mae)\n","    ssim_values.append(validation_ssim)\n","    print(f\"Epoch [{epoch}/{num_epochs}], Validation Loss: {validation_loss:.4f}\")\n","\n","    if epoch % save_every_epoch == 0:\n","        # Save model checkpoints\n","        torch.save(generator.state_dict(), f\"generator_epoch.pt\")\n","        torch.save(discriminator.state_dict(), f\"discriminator_epoch.pt\")\n","\n","        # Generate random image and its colorized version\n","        real_image, grey_image = random.choice(train_loader.dataset)\n","        grey_image = grey_image.unsqueeze(0).to(device)\n","        colorized_image = generator(grey_image)\n","\n","        # Convert tensors to PIL images\n","        grey_image_pil = transforms.ToPILImage()(grey_image.squeeze().cpu())\n","        colorized_image_pil = transforms.ToPILImage()(colorized_image.squeeze().cpu().detach())\n","        original_image_pil = transforms.ToPILImage()(real_image.cpu())\n","\n","        # Plot and save the images\n","        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","        axs[0].imshow(grey_image_pil, cmap='gray')\n","        axs[0].set_title(\"Grayscale Image\")\n","        axs[0].axis(\"off\")\n","        axs[1].imshow(colorized_image_pil)\n","        axs[1].set_title(\"Generated Colorized Image\")\n","        axs[1].axis(\"off\")\n","        axs[2].imshow(original_image_pil)\n","        axs[2].set_title(\"Original RGB Image\")\n","        axs[2].axis(\"off\")\n","        plt.savefig(os.path.join('/kaggle/working/', f\"epoch_{epoch + 1}_colorized_image.png\"))\n","\n","# Plotting\n","plt.figure(figsize=(15, 15))\n","\n","# Training Loss - Discriminator\n","plt.subplot(4, 2, 1)\n","plt.plot(train_losses_discriminator, label='Training Loss - Discriminator')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss - Discriminator')\n","plt.legend()\n","plt.grid(True)\n","\n","# Training Loss - Generator\n","plt.subplot(4, 2, 2)\n","plt.plot(train_losses_generator, label='Training Loss - Generator')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss - Generator')\n","plt.legend()\n","plt.grid(True)\n","\n","# MSE Loss\n","plt.subplot(4, 2, 3)\n","plt.plot(mse_losses, label='MSE Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('MSE Loss')\n","plt.legend()\n","plt.grid(True)\n","\n","# Validation Loss\n","plt.subplot(4, 2, 4)\n","plt.plot(validation_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Validation Loss')\n","plt.legend()\n","plt.grid(True)\n","\n","# PSNR\n","plt.subplot(4, 2, 5)\n","plt.plot(psnr_values, label='PSNR')\n","plt.xlabel('Epoch')\n","plt.ylabel('PSNR')\n","plt.title('PSNR')\n","plt.legend()\n","plt.grid(True)\n","\n","# MAE\n","plt.subplot(4, 2, 6)\n","plt.plot(mae_values, label='MAE')\n","plt.xlabel('Epoch')\n","plt.ylabel('MAE')\n","plt.title('MAE')\n","plt.legend()\n","plt.grid(True)\n","\n","\n","#SSIM\n","plt.subplot(4, 2, 7)\n","plt.plot(ssim_values, label='SSIM')\n","plt.xlabel('Epoch')\n","plt.ylabel('SSIM')\n","plt.title('SSIM')\n","plt.legend()\n","plt.grid(True)\n","\n","\n","plt.tight_layout()\n","plt.savefig('performance_metrics.png')\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T03:56:02.425767Z","iopub.status.busy":"2024-04-17T03:56:02.425438Z","iopub.status.idle":"2024-04-17T03:56:03.184322Z","shell.execute_reply":"2024-04-17T03:56:03.183261Z","shell.execute_reply.started":"2024-04-17T03:56:02.425736Z"},"trusted":true},"outputs":[],"source":["torch.save(generator.state_dict(), f\"generator_epoch.pt\")\n","torch.save(discriminator.state_dict(), f\"discriminator_epoch.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["# Test run"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-17T03:56:03.186728Z","iopub.status.busy":"2024-04-17T03:56:03.186024Z","iopub.status.idle":"2024-04-17T03:56:25.771197Z","shell.execute_reply":"2024-04-17T03:56:25.770144Z","shell.execute_reply.started":"2024-04-17T03:56:03.186698Z"},"trusted":true},"outputs":[],"source":["model = UNetGenerator()\n","model= model.to(device)\n","\n","drive.mount('/content/drive')\n","model_path = '' # replace path here\n","model.load_state_dict(torch.load(model_path))\n","\n","model.eval()\n","for i, (real_image, gray_image) in enumerate(test_loader):\n","\n","    with torch.no_grad():\n","        # Generate random image and its colorized version\n","        gray_image = gray_image.to(device)\n","        colorized_image = model(gray_image)\n","        colorized_image = colorized_image[0]\n","\n","        # Convert tensors to PIL images\n","        gray_image_pil = transforms.ToPILImage()(gray_image[0].cpu())\n","        colorized_image_pil = transforms.ToPILImage()(colorized_image.cpu().detach())\n","        original_image_pil = transforms.ToPILImage()(real_image[0].cpu())\n","\n","        # Plot and save the images\n","        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","        axs[0].imshow(gray_image_pil, cmap='gray')\n","        axs[0].set_title(\"Grayscale Image\")\n","        axs[0].axis(\"off\")\n","        axs[1].imshow(colorized_image_pil)\n","        axs[1].set_title(\"Generated Colorized Image\")\n","        axs[1].axis(\"off\")\n","        axs[2].imshow(original_image_pil)\n","        axs[2].set_title(\"Original RGB Image\")\n","        axs[2].axis(\"off\")\n","\n","        if i == 6:\n","            break"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4805002,"sourceId":8129637,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
